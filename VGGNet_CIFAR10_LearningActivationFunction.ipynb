{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGGNet_CIFAR10_LearningActivationFunction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "E6zv2XtJLjWv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10 using VGGNet"
      ]
    },
    {
      "metadata": {
        "id": "w0GBluQ5PDsP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Learning Activation Function used: https://arxiv.org/abs/1412.6830"
      ]
    },
    {
      "metadata": {
        "id": "uJKgxe7fLjWx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Load dependencies"
      ]
    },
    {
      "metadata": {
        "id": "-1bn3KikLjWy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(42)\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HJqikB-KLjW2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Load data"
      ]
    },
    {
      "metadata": {
        "id": "xYkDj0fyLjW3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "train_y = to_categorical(train_y)\n",
        "test_y = to_categorical(test_y)\n",
        "\n",
        "val_x = train_x[40000:50000]\n",
        "val_y = train_y[40000:50000]\n",
        "\n",
        "train_x = train_x[0:40000]\n",
        "train_y = train_y[0:40000]\n",
        "\n",
        "train_x = train_x/255\n",
        "test_x = test_x/255\n",
        "val_x = val_x/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PedEMx-ALjW-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Set parameters for each layer"
      ]
    },
    {
      "metadata": {
        "id": "hcKRrKG0LjXA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# input layer: \n",
        "n_input = 784\n",
        "\n",
        "# first convolutional layer: \n",
        "n_conv_1 = 64\n",
        "k_conv_1 = 3 # k_size\n",
        "\n",
        "# second convolutional layer: \n",
        "n_conv_2 = 64\n",
        "k_conv_2 = 3\n",
        "\n",
        "n_conv_3 = 128\n",
        "k_conv_3 = 3\n",
        "\n",
        "n_conv_4 = 128\n",
        "k_conv_4 = 3\n",
        "\n",
        "n_conv_5 = 256\n",
        "k_conv_5 = 3\n",
        "\n",
        "n_conv_6 = 256\n",
        "k_conv_6 = 3\n",
        "\n",
        "n_conv_7 = 256\n",
        "k_conv_7 = 3\n",
        "\n",
        "n_conv_8 = 512\n",
        "k_conv_8 = 3\n",
        "\n",
        "n_conv_9 = 512\n",
        "k_conv_9 = 3\n",
        "\n",
        "n_conv_10 = 512\n",
        "k_conv_10 = 3\n",
        "\n",
        "n_conv_11 = 512\n",
        "k_conv_11 = 3\n",
        "\n",
        "n_conv_12 = 512\n",
        "k_conv_12 = 3\n",
        "\n",
        "n_conv_13 = 512\n",
        "k_conv_13 = 3\n",
        "\n",
        "n_dense = 512\n",
        "\n",
        "\n",
        "# max pooling layer:\n",
        "pool_size = 2\n",
        "dropout_1 = 0.3\n",
        "dropout_2 = 0.4\n",
        "dropout_3 = 0.5\n",
        "\n",
        "# output layer: \n",
        "n_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R9j9O3CuLjW8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "display_progress = 40 # after this many batches, output progress to screen\n",
        "wt_init = tf.contrib.layers.xavier_initializer() # weight initializer\n",
        "epoch_no_change = 400 # stops training if validation loss doesn't decrease in the last \"epoch_no_change\" epochs\n",
        "restart_training = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NweTYiyRLjXB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define placeholder Tensors for inputs and labels"
      ]
    },
    {
      "metadata": {
        "id": "eGvhu4PzLjXC",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32, [None, 32,32,3])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "do_dropout = tf.placeholder(tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bhm-AJD7jSY8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xs_arr1 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "ys_arr1 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "xs_arr2 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "ys_arr2 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "xs_arr3 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "ys_arr3 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "xs_arr4 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "ys_arr4 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "xs_arr5 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "ys_arr5 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "xs_arr6 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "ys_arr6 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "xs_arr7 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "ys_arr7 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "xs_arr8 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "ys_arr8 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "xs_arr9 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "ys_arr9 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "xs_arr10 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "ys_arr10 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "xs_arr11 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "ys_arr11 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "xs_arr12 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "ys_arr12 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "xs_arr13 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "ys_arr13 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "xs_arr14 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "ys_arr14 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "\n",
        "\n",
        "nxs_arr1 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nys_arr1 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nxs_arr2 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nys_arr2 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nxs_arr3 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nys_arr3 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nxs_arr4 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nys_arr4 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nxs_arr5 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nys_arr5 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nxs_arr6 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nys_arr6 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nxs_arr7 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nys_arr7 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nxs_arr8 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nys_arr8 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nxs_arr9 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nys_arr9 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nxs_arr10 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nys_arr10 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nxs_arr11 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nys_arr11 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nxs_arr12 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nys_arr12 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nxs_arr13 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nys_arr13 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nxs_arr14 = np.array([-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "nys_arr14 = np.array([0.0,0.0,0.0,0.0,0.5,1.0,2.0],dtype=np.float32)\n",
        "\n",
        "xs_dict = {\n",
        "    '1': xs_arr1,\n",
        "    '2': xs_arr2,\n",
        "    '3': xs_arr3,\n",
        "    '4': xs_arr4,\n",
        "    '5': xs_arr5,\n",
        "    '6': xs_arr6,\n",
        "    '7': xs_arr7,\n",
        "    '8': xs_arr8,\n",
        "    '9': xs_arr9,\n",
        "    '10': xs_arr10,\n",
        "    '11': xs_arr11,\n",
        "    '12': xs_arr12,\n",
        "    '13': xs_arr13,\n",
        "    '14': xs_arr14,\n",
        "}\n",
        "\n",
        "ys_dict = {\n",
        "    '1': ys_arr1,\n",
        "    '2': ys_arr2,\n",
        "    '3': ys_arr3,\n",
        "    '4': ys_arr4,\n",
        "    '5': ys_arr5,\n",
        "    '6': ys_arr6,\n",
        "    '7': ys_arr7,\n",
        "    '8': ys_arr8,\n",
        "    '9': ys_arr9,\n",
        "    '10': ys_arr10,\n",
        "    '11': ys_arr11,\n",
        "    '12': ys_arr12,\n",
        "    '13': ys_arr13,\n",
        "    '14': ys_arr14,\n",
        "}\n",
        "\n",
        "nxs_dict = {\n",
        "    '1': nxs_arr1,\n",
        "    '2': nxs_arr2,\n",
        "    '3': nxs_arr3,\n",
        "    '4': nxs_arr4,\n",
        "    '5': nxs_arr5,\n",
        "    '6': nxs_arr6,\n",
        "    '7': nxs_arr7,\n",
        "    '8': nxs_arr8,\n",
        "    '9': nxs_arr9,\n",
        "    '10': nxs_arr10,\n",
        "    '11': nxs_arr11,\n",
        "    '12': nxs_arr12,\n",
        "    '13': nxs_arr13,\n",
        "    '14': nxs_arr14,\n",
        "}\n",
        "\n",
        "nys_dict = {\n",
        "    '1': nys_arr1,\n",
        "    '2': nys_arr2,\n",
        "    '3': nys_arr3,\n",
        "    '4': nys_arr4,\n",
        "    '5': nys_arr5,\n",
        "    '6': nys_arr6,\n",
        "    '7': nys_arr7,\n",
        "    '8': nys_arr8,\n",
        "    '9': nys_arr9,\n",
        "    '10': nys_arr10,\n",
        "    '11': nys_arr11,\n",
        "    '12': nys_arr12,\n",
        "    '13': nys_arr13,\n",
        "    '14': nys_arr14,\n",
        "}\n",
        "\n",
        "m_dict = {\n",
        "    '1': xs_arr1,\n",
        "    '2': xs_arr2,\n",
        "    '3': xs_arr3,\n",
        "    '4': xs_arr4,\n",
        "    '5': xs_arr5,\n",
        "    '6': xs_arr6,\n",
        "    '7': xs_arr7,\n",
        "    '8': xs_arr8,\n",
        "    '9': xs_arr9,\n",
        "    '10': xs_arr10,\n",
        "    '11': xs_arr11,\n",
        "    '12': xs_arr12,\n",
        "    '13': xs_arr13,\n",
        "    '14': xs_arr14,\n",
        "}\n",
        "\n",
        "c_dict = {\n",
        "    '1': xs_arr1,\n",
        "    '2': xs_arr2,\n",
        "    '3': xs_arr3,\n",
        "    '4': xs_arr4,\n",
        "    '5': xs_arr5,\n",
        "    '6': xs_arr6,\n",
        "    '7': xs_arr7,\n",
        "    '8': xs_arr8,\n",
        "    '9': xs_arr9,\n",
        "    '10': xs_arr10,\n",
        "    '11': xs_arr11,\n",
        "    '12': xs_arr12,\n",
        "    '13': xs_arr13,\n",
        "    '14': xs_arr14,\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uIHG1FMKLjXE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define types of layers"
      ]
    },
    {
      "metadata": {
        "id": "WAasaA1RLjXE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# dense layer with ReLU activation:\n",
        "def dense(x, W, b, s):\n",
        "    x=tf.Print(x,[x],\"DENSE X \")\n",
        "    z = tf.add(tf.matmul(x, W), b)\n",
        "    a = custom_activation(z,s)\n",
        "    #a=tf.nn.relu(z)\n",
        "    return a\n",
        "\n",
        "# convolutional layer with ReLU activation:\n",
        "def conv2d(x, W, b, s,stride_length=1):\n",
        "    x=tf.Print(x,[x],\"CONV2D X \")\n",
        "    xW = tf.nn.conv2d(x, W, strides=[1, stride_length, stride_length, 1], padding='SAME')\n",
        "    z = tf.nn.bias_add(xW, b)\n",
        "    a = custom_activation(z,s)\n",
        "    #a=tf.nn.relu(z)\n",
        "    return a\n",
        "\n",
        "# max-pooling layer: \n",
        "def maxpooling2d(x, p_size):\n",
        "    return tf.nn.max_pool(x, \n",
        "                          ksize=[1, p_size, p_size, 1], \n",
        "                          strides=[1, p_size, p_size, 1], \n",
        "                          padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FQX4NyhALjXH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Design neural network architecture"
      ]
    },
    {
      "metadata": {
        "id": "lE041gBJLjXI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def network(x, weights, biases, n_in, mp_psize, dropout_1, dropout_2, dropout_3, do_dropout):\n",
        "    square_x = x\n",
        "    \n",
        "    conv_1 = conv2d(square_x, weights['W_c1'], biases['b_c1'],'1')\n",
        "    conv_1 = tf.nn.dropout(conv_1, 1-tf.multiply(dropout_1,do_dropout))\n",
        "    \n",
        "    conv_2 = conv2d(conv_1, weights['W_c2'], biases['b_c2'],'2')\n",
        "    conv_2 = tf.nn.max_pool(conv_2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "    #conv_1 = tf.nn.dropout(conv_1, 1-tf.multiply(dropout_1,do_dropout))\n",
        "    \n",
        "    conv_3 = conv2d(conv_2, weights['W_c3'], biases['b_c3'],'3')\n",
        "    conv_3 = tf.nn.dropout(conv_3, 1-tf.multiply(dropout_2,do_dropout))\n",
        "    \n",
        "    conv_4 = conv2d(conv_3, weights['W_c4'], biases['b_c4'],'4')\n",
        "    conv_4 = tf.nn.max_pool(conv_4,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "    #conv_1 = tf.nn.dropout(conv_1, 1-tf.multiply(dropout_1,do_dropout))\n",
        "\n",
        "    conv_5 = conv2d(conv_4, weights['W_c5'], biases['b_c5'],'5')\n",
        "    conv_5 = tf.nn.dropout(conv_5, 1-tf.multiply(dropout_2,do_dropout))\n",
        "\n",
        "    conv_6 = conv2d(conv_5, weights['W_c6'], biases['b_c6'],'6')\n",
        "    conv_6 = tf.nn.dropout(conv_6, 1-tf.multiply(dropout_2,do_dropout))\n",
        "\n",
        "    conv_7 = conv2d(conv_6, weights['W_c7'], biases['b_c7'],'7')\n",
        "    conv_7 = tf.nn.max_pool(conv_7,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "    #conv_1 = tf.nn.dropout(conv_1, 1-tf.multiply(dropout_1,do_dropout))\n",
        "\n",
        "    conv_8 = conv2d(conv_7, weights['W_c8'], biases['b_c8'],'8')\n",
        "    conv_8 = tf.nn.dropout(conv_8, 1-tf.multiply(dropout_2,do_dropout))\n",
        "\n",
        "    conv_9 = conv2d(conv_8, weights['W_c9'], biases['b_c9'],'9')\n",
        "    conv_9 = tf.nn.dropout(conv_9, 1-tf.multiply(dropout_2,do_dropout))\n",
        "    \n",
        "    conv_10 = conv2d(conv_9, weights['W_c10'], biases['b_c10'],'10')\n",
        "    conv_10 = tf.nn.max_pool(conv_10,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "\n",
        "    ##conv_1 = tf.nn.dropout(conv_1, 1-tf.multiply(dropout_1,do_dropout))\n",
        "\n",
        "    conv_11 = conv2d(conv_10, weights['W_c11'], biases['b_c11'],'11')\n",
        "    conv_11 = tf.nn.dropout(conv_11, 1-tf.multiply(dropout_2,do_dropout))\n",
        "\n",
        "    conv_12 = conv2d(conv_11, weights['W_c12'], biases['b_c12'],'12')\n",
        "    conv_12 = tf.nn.dropout(conv_12, 1-tf.multiply(dropout_2,do_dropout))\n",
        "\n",
        "    conv_13 = conv2d(conv_12 , weights['W_c13'], biases['b_c13'],'13')\n",
        "    conv_13 = tf.nn.max_pool(conv_13,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "    conv_13 = tf.nn.dropout(conv_13, 1-tf.multiply(dropout_3,do_dropout))\n",
        "    \n",
        "    flat = tf.reshape(conv_13, [-1, weights['W_d1'].get_shape().as_list()[0]])\n",
        "    dense_1 = dense(flat, weights['W_d1'], biases['b_d1'],'14')\n",
        "    dense_1 = tf.nn.dropout(dense_1, 1-tf.multiply(dropout_3,do_dropout))\n",
        "    \n",
        "    # output layer: \n",
        "    out_layer_z = tf.add(tf.matmul(dense_1, weights['W_out']), biases['b_out'])\n",
        "    \n",
        "    return out_layer_z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJWDx6siLjXL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define dictionaries for storing weights and biases for each layer -- and initialize"
      ]
    },
    {
      "metadata": {
        "id": "9s6JNhn2LjXM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# calculate number of inputs to dense layer: \n",
        "full_square_length = np.sqrt(n_input)\n",
        "pooled_square_length = int(full_square_length / pool_size)\n",
        "dense_inputs = (pooled_square_length**2 * n_conv_2)/4\n",
        "\n",
        "\n",
        "if(restart_training==1):\n",
        "  bias_dict = {\n",
        "      'b_c1': tf.Variable(tf.zeros([n_conv_1])),\n",
        "      'b_c2': tf.Variable(tf.zeros([n_conv_2])),\n",
        "      'b_c3': tf.Variable(tf.zeros([n_conv_3])),\n",
        "      'b_c4': tf.Variable(tf.zeros([n_conv_4])),\n",
        "      'b_c5': tf.Variable(tf.zeros([n_conv_5])),\n",
        "      'b_c6': tf.Variable(tf.zeros([n_conv_6])),\n",
        "      'b_c7': tf.Variable(tf.zeros([n_conv_7])),\n",
        "      'b_c8': tf.Variable(tf.zeros([n_conv_8])),\n",
        "      'b_c9': tf.Variable(tf.zeros([n_conv_9])),\n",
        "      'b_c10': tf.Variable(tf.zeros([n_conv_10])),\n",
        "      'b_c11': tf.Variable(tf.zeros([n_conv_11])),\n",
        "      'b_c12': tf.Variable(tf.zeros([n_conv_12])),\n",
        "      'b_c13': tf.Variable(tf.zeros([n_conv_13])),\n",
        "      'b_d1': tf.Variable(tf.zeros([n_dense])),\n",
        "      'b_out': tf.Variable(tf.zeros([n_classes]))\n",
        "  }\n",
        "\n",
        "  weight_dict = {\n",
        "      'W_c1': tf.get_variable('W_c1', [k_conv_1, k_conv_1, 3,n_conv_1], initializer=wt_init, regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c2': tf.get_variable('W_c2', [k_conv_2, k_conv_2, n_conv_1,n_conv_2], initializer=wt_init, regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c3': tf.get_variable('W_c3', [k_conv_3, k_conv_3, n_conv_2,n_conv_3], initializer=wt_init, regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c4': tf.get_variable('W_c4', [k_conv_4, k_conv_4, n_conv_3,n_conv_4], initializer=wt_init, regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c5': tf.get_variable('W_c5', [k_conv_5, k_conv_5, n_conv_4,n_conv_5], initializer=wt_init, regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c6': tf.get_variable('W_c6', [k_conv_6, k_conv_6, n_conv_5,n_conv_6], initializer=wt_init, regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c7': tf.get_variable('W_c7', [k_conv_7, k_conv_7, n_conv_6,n_conv_7], initializer=wt_init, regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c8': tf.get_variable('W_c8', [k_conv_8, k_conv_8, n_conv_7,n_conv_8], initializer=wt_init, regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c9': tf.get_variable('W_c9', [k_conv_9, k_conv_9, n_conv_8,n_conv_9], initializer=wt_init, regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c10': tf.get_variable('W_c10', [k_conv_10, k_conv_10, n_conv_9,n_conv_10], initializer=wt_init, regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c11': tf.get_variable('W_c11', [k_conv_11, k_conv_11, n_conv_10,n_conv_11], initializer=wt_init, regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c12': tf.get_variable('W_c12', [k_conv_12, k_conv_12, n_conv_11,n_conv_12], initializer=wt_init, regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c13': tf.get_variable('W_c13', [k_conv_13, k_conv_13, n_conv_12,n_conv_13], initializer=wt_init, regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_d1': tf.get_variable('W_d1', [512, n_dense], initializer=wt_init, regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),      \n",
        "      'W_out': tf.get_variable('W_out', [n_dense, n_classes], initializer=wt_init)\n",
        "      \n",
        "  }\n",
        "  \n",
        "else:\n",
        "  \n",
        "  W_c1 = np.load('W_c1.npy')\n",
        "  W_c2 = np.load('W_c2.npy')\n",
        "  W_c3 = np.load('W_c3.npy')\n",
        "  W_c4 = np.load('W_c4.npy')\n",
        "  W_c5 = np.load('W_c5.npy')\n",
        "  W_c6 = np.load('W_c6.npy')\n",
        "  W_c7 = np.load('W_c7.npy')\n",
        "  W_c8 = np.load('W_c8.npy')\n",
        "  W_c9 = np.load('W_c9.npy')\n",
        "  W_c10 = np.load('W_c10.npy')\n",
        "  W_c11 = np.load('W_c11.npy')\n",
        "  W_c12 = np.load('W_c12.npy')\n",
        "  W_c13 = np.load('W_c13.npy')\n",
        "  W_d1 = np.load('W_d1.npy')\n",
        "  W_out = np.load('W_out.npy')\n",
        "  \n",
        "  b_c1 = np.load('b_c1.npy')\n",
        "  b_c2 = np.load('b_c2.npy')\n",
        "  b_c3 = np.load('b_c3.npy')\n",
        "  b_c4 = np.load('b_c4.npy')\n",
        "  b_c5 = np.load('b_c5.npy')\n",
        "  b_c6 = np.load('b_c6.npy')\n",
        "  b_c7 = np.load('b_c7.npy')\n",
        "  b_c8 = np.load('b_c8.npy')\n",
        "  b_c9 = np.load('b_c9.npy')\n",
        "  b_c10 = np.load('b_c10.npy')\n",
        "  b_c11 = np.load('b_c11.npy')\n",
        "  b_c12 = np.load('b_c12.npy')\n",
        "  b_c13 = np.load('b_c13.npy')\n",
        "  b_d1 = np.load('b_d1.npy')\n",
        "  b_out = np.load('b_out.npy')\n",
        "\n",
        "  bias_dict = {\n",
        "      'b_c1': tf.Variable(b_c1),\n",
        "      'b_c2': tf.Variable(b_c2),\n",
        "      'b_c3': tf.Variable(b_c3),\n",
        "      'b_c4': tf.Variable(b_c4),\n",
        "      'b_c5': tf.Variable(b_c5),\n",
        "      'b_c6': tf.Variable(b_c6),\n",
        "      'b_c7': tf.Variable(b_c7),\n",
        "      'b_c8': tf.Variable(b_c8),\n",
        "      'b_c9': tf.Variable(b_c9),\n",
        "      'b_c10': tf.Variable(b_c10),\n",
        "      'b_c11': tf.Variable(b_c11),\n",
        "      'b_c12': tf.Variable(b_c12),\n",
        "      'b_c13': tf.Variable(b_c13),\n",
        "      'b_d1': tf.Variable(b_d1),\n",
        "      'b_out': tf.Variable(b_out)\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "  weight_dict = {\n",
        "      'W_c1': tf.get_variable(\"W_c1\", initializer=tf.constant_initializer(W_c1), regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c2': tf.get_variable(\"W_c2\", initializer=tf.constant_initializer(W_c2), regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c3': tf.get_variable(\"W_c3\", initializer=tf.constant_initializer(W_c3), regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c4': tf.get_variable(\"W_c4\", initializer=tf.constant_initializer(W_c4), regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c5': tf.get_variable(\"W_c5\", initializer=tf.constant_initializer(W_c5), regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c6': tf.get_variable(\"W_c6\", initializer=tf.constant_initializer(W_c6), regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c7': tf.get_variable(\"W_c7\", initializer=tf.constant_initializer(W_c7), regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c8': tf.get_variable(\"W_c8\", initializer=tf.constant_initializer(W_c8), regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c9': tf.get_variable(\"W_c9\", initializer=tf.constant_initializer(W_c9), regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c10': tf.get_variable(\"W_c10\", initializer=tf.constant_initializer(W_c10), regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c11': tf.get_variable(\"W_c11\", initializer=tf.constant_initializer(W_c11), regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c12': tf.get_variable(\"W_c12\", initializer=tf.constant_initializer(W_c12), regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_c13': tf.get_variable(\"W_c13\", initializer=tf.constant_initializer(W_c13), regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_d1': tf.get_variable(\"W_d1\", initializer=tf.constant_initializer(W_d1), regularizer=tf.contrib.layers.l2_regularizer(scale=0.0005)),\n",
        "      'W_out': tf.Variable(W_out)\n",
        "      \n",
        "  }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RFrpQp6jLjXO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Learning Activation Function"
      ]
    },
    {
      "metadata": {
        "id": "DtZcmtITNBHR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.framework import ops\n",
        "initializer_op = tf.global_variables_initializer()\n",
        "session = tf.Session()\n",
        "session.run(initializer_op)\n",
        "\n",
        "\n",
        "def pls(xs,ys,m,c,ota,otb,a2):\n",
        "  for i in range(xs.shape[0]-1):\n",
        "    m[i] -= ota[i]\n",
        "    c[i] -= otb[i]\n",
        "  xs2 = xs\n",
        "  ys2 = ys\n",
        "  ys2[0] = m[0]*xs2[0] + c[0]\n",
        "  \n",
        "  if(np.isnan(ys2[0])):\n",
        "    print()\n",
        "    print(\"GRAD = \",np.sum(np.isnan(grad)),\" X = \",np.sum(np.isnan(x)))\n",
        "    print(\"shape = \",x.shape)\n",
        "    print()\n",
        "  for i in range(1,xs.shape[0]-1):\n",
        "    ys2[i] = ( (m[i-1]*xs2[i] + c[i-1]) + (m[i]*xs2[i] + c[i]) )/2.0\n",
        "  ys2[xs.shape[0]-1] = m[xs.shape[0]-2]*xs2[xs.shape[0]-1] + c[xs.shape[0]-2]\n",
        "  nys_dict[str(a2)] += ys2  \n",
        "  return np.array([0.0],dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "def np_custom_activation_grad(x,s,i2,grad):\n",
        "  i = tf.cast(i2,tf.int32)\n",
        "  a2 = tf.cast(tf.reduce_max(s),dtype=tf.int32)\n",
        "  tf_xs = tf.cast(tf.py_func(lambda val: np.float32(xs_dict[str(val)]), [a2], tf.float32),dtype=tf.float32)\n",
        "  tf_ys = tf.cast(tf.py_func(lambda val: np.float32(ys_dict[str(val)]), [a2], tf.float32),dtype=tf.float32)\n",
        "  \n",
        "  tf_m2 = tf.cast(tf.py_func(lambda val: np.float32(m_dict[str(val)]), [a2], tf.float32),dtype=tf.float32)\n",
        "  tf_c2 = tf.cast(tf.py_func(lambda val: np.float32(c_dict[str(val)]), [a2], tf.float32),dtype=tf.float32)\n",
        "  slope = tf.gather(tf_m2,i)\n",
        "  y = tf.multiply(grad,slope)\n",
        "  \n",
        "  az = tf.zeros([tf.shape(tf_xs)[0]-1],dtype=tf.float32)\n",
        "  bz = tf.zeros([tf.shape(tf_xs)[0]-1],dtype=tf.float32)\n",
        "  az0 = tf.zeros([1],dtype=tf.float32)\n",
        "  bz0 = tf.zeros([1],dtype=tf.float32)\n",
        "  \n",
        "  tf_az = tf.Variable(az0,dtype=tf.float32)\n",
        "  tf_bz = tf.Variable(bz0,dtype=tf.float32)\n",
        "  tf_az = tf.assign(tf_az, az, validate_shape=False)\n",
        "  tf_bz = tf.assign(tf_bz, bz, validate_shape=False)\n",
        "  \n",
        "  curve_lr = 0.001\n",
        "  tf_az = tf.scatter_add(tf_az,i,tf.multiply(tf.multiply(grad,x),curve_lr))\n",
        "  tf_bz = tf.scatter_add(tf_bz,i,tf.multiply(grad,curve_lr))\n",
        "  ad = tf.py_func(pls,[tf_xs,tf_ys,tf_m2,tf_c2,tf_az,tf_bz,a2],[tf.float32])\n",
        "  y = tf.add(y,ad)\n",
        "  \n",
        "  return y,y,y\n",
        "  \n",
        "\n",
        "    \n",
        "    \n",
        "@tf.RegisterGradient(\"np_custom_activation\")\n",
        "def custom_activation_grad(op,grad):  \n",
        "  return np_custom_activation_grad(op.inputs[0],op.inputs[1],op.inputs[2],grad)\n",
        "\n",
        "\n",
        "\n",
        "def custom_activation(tf_x,s):\n",
        "  s2 = tf.string_to_number(s)\n",
        "  zeros = tf.subtract(tf_x,tf_x)\n",
        "  s3 = tf.add(zeros,tf.cast(s2,tf.float32))\n",
        "  g = tf.get_default_graph()\n",
        "  a2 = tf.cast(tf.reduce_max(s2),dtype=tf.int32)\n",
        "  \n",
        "  tf_xs = tf.cast(tf.py_func(lambda val: np.float32(xs_dict[str(val)]), [a2], tf.float32),dtype=tf.float32)\n",
        "  tf_ys = tf.cast(tf.py_func(lambda val: np.float32(ys_dict[str(val)]), [a2], tf.float32),dtype=tf.float32)\n",
        "  x_sh = tf_x.get_shape().as_list()\n",
        "  tf_x_len = len(tf_x.get_shape().as_list())\n",
        "  sh = tf.ones(tf_x_len)\n",
        "\n",
        "  tf_xc = tf.expand_dims(tf_x,tf_x_len)\n",
        "\n",
        "  sh = tf.concat([sh,tf.cast(tf.shape(tf_xs),dtype=tf.float32)],axis=0)\n",
        "  sh = tf.cast(sh,tf.int32)\n",
        "\n",
        "  tf_x2 = tf.tile(tf_xc, sh)\n",
        "\n",
        "  xdf = tf.abs(tf.subtract(tf_x2, tf_xs))\n",
        "  i = tf.cast(tf.argmin(xdf,axis=tf_x_len),tf.int32)\n",
        "  tf_l = tf.cast(tf.greater(tf.gather(tf_xs,i), tf_x),dtype=tf.int32)\n",
        "  i = tf.subtract(i,tf_l)\n",
        "  i2 = tf.cast(i,tf.float32)\n",
        "  with g.gradient_override_map({\"AddN\": \"np_custom_activation\"}):\n",
        "    tf_x21 = tf.add_n([tf_x,s3,i2])\n",
        "  tf_x22 = tf.subtract(tf_x21,s3)\n",
        "  tf_x22 = tf.subtract(tf_x22,i2)\n",
        "  slope = tf.gather(tf.cast(tf.py_func(lambda val: np.float32(m_dict[str(val)]), [a2], tf.float32),dtype=tf.float32),i)\n",
        "  c = tf.gather(tf.cast(tf.py_func(lambda val: np.float32(c_dict[str(val)]), [a2], tf.float32),dtype=tf.float32),i)\n",
        "  y = tf.add(tf.multiply(tf_x22,slope),c)\n",
        "  return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NHW5Ju9OLjXR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build model"
      ]
    },
    {
      "metadata": {
        "id": "G0eM_X7DVS-w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = network(x, weight_dict, bias_dict, n_input, \n",
        "                      pool_size, dropout_1, dropout_2, dropout_3, do_dropout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PwzPJOSfK2wr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define model's loss and its optimizer"
      ]
    },
    {
      "metadata": {
        "id": "bPC4r2i7LjXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\n",
        "batch = tf.Variable(0)\n",
        "\n",
        "learning_rate = tf.train.exponential_decay(\n",
        "  0.01,                # Base learning rate.\n",
        "  batch * batch_size,  # Current index into the dataset.\n",
        "  500,          # Decay step.\n",
        "  0.999999,                # Decay rate.\n",
        "  staircase=True)\n",
        "# Use simple momentum for the optimization.\n",
        "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
        "                                     0.9).minimize(cost,\n",
        "                                                   global_step=batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W0nA9RDxLjXU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define evaluation metrics"
      ]
    },
    {
      "metadata": {
        "id": "3xVxDPijLjXV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
        "accuracy_pct = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YFGHAUj2LjXZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Create op for variable initialization"
      ]
    },
    {
      "metadata": {
        "id": "5__x9YTELjXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "initializer_op = tf.global_variables_initializer()\n",
        "session.run(initializer_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_f8V9q_LAxZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Initialize Variables"
      ]
    },
    {
      "metadata": {
        "id": "X9pIVU4EwGRT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if(restart_training==1):\n",
        "  wd,bd = session.run([weight_dict,bias_dict])\n",
        "\n",
        "\n",
        "  epoch_arr = np.zeros(1)\n",
        "  cost_arr = np.zeros(1)\n",
        "  accuracy_arr = np.zeros(1)\n",
        "  train_cost_arr = np.zeros(1)\n",
        "  train_accuracy_arr = np.zeros(1)\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  np.save(\"W_c1.npy\",wd['W_c1'])\n",
        "  np.save(\"W_c2.npy\",wd['W_c2'])\n",
        "  np.save(\"W_c3.npy\",wd['W_c3'])\n",
        "  np.save(\"W_c4.npy\",wd['W_c4'])\n",
        "  np.save(\"W_c5.npy\",wd['W_c5'])\n",
        "  np.save(\"W_c6.npy\",wd['W_c6'])\n",
        "  np.save(\"W_c7.npy\",wd['W_c7'])\n",
        "  np.save(\"W_c8.npy\",wd['W_c8'])\n",
        "  np.save(\"W_c9.npy\",wd['W_c9'])\n",
        "  np.save(\"W_c10.npy\",wd['W_c10'])\n",
        "  np.save(\"W_c11.npy\",wd['W_c11'])\n",
        "  np.save(\"W_c12.npy\",wd['W_c12'])\n",
        "  np.save(\"W_c13.npy\",wd['W_c13'])\n",
        "  np.save(\"W_d1.npy\",wd['W_d1'])\n",
        "  np.save(\"W_out.npy\",wd['W_out'])\n",
        "  np.save(\"b_c1.npy\",bd['b_c1'])\n",
        "  np.save(\"b_c2.npy\",bd['b_c2'])\n",
        "  np.save(\"b_c3.npy\",bd['b_c3'])\n",
        "  np.save(\"b_c4.npy\",bd['b_c4'])\n",
        "  np.save(\"b_c5.npy\",bd['b_c5'])\n",
        "  np.save(\"b_c6.npy\",bd['b_c6'])\n",
        "  np.save(\"b_c7.npy\",bd['b_c7'])\n",
        "  np.save(\"b_c8.npy\",bd['b_c8'])\n",
        "  np.save(\"b_c9.npy\",bd['b_c9'])\n",
        "  np.save(\"b_c10.npy\",bd['b_c10'])\n",
        "  np.save(\"b_c11.npy\",bd['b_c11'])\n",
        "  np.save(\"b_c12.npy\",bd['b_c12'])\n",
        "  np.save(\"b_c13.npy\",bd['b_c13'])\n",
        "  np.save(\"b_d1.npy\",bd['b_d1'])\n",
        "  np.save(\"b_out.npy\",bd['b_out'])\n",
        "  np.save(\"xs_arr1.npy\",xs_arr1)\n",
        "  np.save(\"ys_arr1.npy\",ys_arr1)\n",
        "  np.save(\"xs_arr2.npy\",xs_arr2)\n",
        "  np.save(\"ys_arr2.npy\",ys_arr2)\n",
        "  np.save(\"xs_arr3.npy\",xs_arr3)\n",
        "  np.save(\"ys_arr3.npy\",ys_arr3)\n",
        "  np.save(\"xs_arr4.npy\",xs_arr4)\n",
        "  np.save(\"ys_arr4.npy\",ys_arr4)\n",
        "  np.save(\"xs_arr5.npy\",xs_arr5)\n",
        "  np.save(\"ys_arr5.npy\",ys_arr5)\n",
        "  np.save(\"xs_arr6.npy\",xs_arr6)\n",
        "  np.save(\"ys_arr6.npy\",ys_arr6)\n",
        "  np.save(\"xs_arr7.npy\",xs_arr7)\n",
        "  np.save(\"ys_arr7.npy\",ys_arr7)\n",
        "  np.save(\"xs_arr8.npy\",xs_arr8)\n",
        "  np.save(\"ys_arr8.npy\",ys_arr8)\n",
        "  np.save(\"xs_arr9.npy\",xs_arr9)\n",
        "  np.save(\"ys_arr9.npy\",ys_arr9)\n",
        "  np.save(\"xs_arr10.npy\",xs_arr10)\n",
        "  np.save(\"ys_arr10.npy\",ys_arr10)\n",
        "  np.save(\"xs_arr11.npy\",xs_arr11)\n",
        "  np.save(\"ys_arr11.npy\",ys_arr11)\n",
        "  np.save(\"xs_arr12.npy\",xs_arr12)\n",
        "  np.save(\"ys_arr12.npy\",ys_arr12)\n",
        "  np.save(\"xs_arr13.npy\",xs_arr13)\n",
        "  np.save(\"ys_arr13.npy\",ys_arr13)\n",
        "  np.save(\"xs_arr14.npy\",xs_arr14)\n",
        "  np.save(\"ys_arr14.npy\",ys_arr14)\n",
        "  np.save(\"epochs.npy\",epoch_arr)\n",
        "  np.save(\"cost.npy\",cost_arr)\n",
        "  np.save(\"accuracy.npy\",accuracy_arr)\n",
        "  np.save(\"train_cost.npy\",train_cost_arr)\n",
        "  np.save(\"train_accuracy.npy\",train_accuracy_arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fJENLHFYLG53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Next Batch (Helper Function)"
      ]
    },
    {
      "metadata": {
        "id": "96SUFOzfQ9H3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Return a total of `num` random samples and labels.\n",
        "def next_batch(num, data, labels):\n",
        "  idx = np.arange(0 , len(data))\n",
        "  np.random.shuffle(idx)\n",
        "  idx = idx[:num]\n",
        "  data_shuffle = [data[ i] for i in idx]\n",
        "  labels_shuffle = [labels[ i] for i in idx]\n",
        "  return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wVlIuW10LI2Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Augment Images (Helper Functions)"
      ]
    },
    {
      "metadata": {
        "id": "Cs6203qRu3BK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from math import ceil, floor, pi\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def rotate_images(X_imgs, max_angle):\n",
        "    n_images = len(X_imgs)\n",
        "    angles = np.random.randint(0,2*max_angle,n_images)\n",
        "    angles = angles - max_angle\n",
        "    angles = angles*np.pi/180\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    X = tf.placeholder(tf.float32, shape=(len(X_imgs), 32, 32, 3))\n",
        "    radian = tf.placeholder(tf.float32, shape=(len(X_imgs)))\n",
        "    tf_img = tf.contrib.image.rotate(X, radian)\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        rotated_imgs = sess.run(tf_img, feed_dict={X: X_imgs, radian: angles})\n",
        "\n",
        "    X_rotate = rotated_imgs\n",
        "    return X_rotate\n",
        "\n",
        "  \n",
        "def get_translate_parameters(X_imgs, index, amount):\n",
        "    IMAGE_SIZE = 32\n",
        "    if index == 0: # Translate left 20 percent\n",
        "        offset = np.array([0.0, amount], dtype = np.float32)\n",
        "        size = np.array([IMAGE_SIZE, ceil((1-amount) * IMAGE_SIZE)], dtype = np.int32)\n",
        "        w_start = 0\n",
        "        w_end = int(ceil((1-amount) * IMAGE_SIZE))\n",
        "        h_start = 0\n",
        "        h_end = IMAGE_SIZE\n",
        "    elif index == 1: # Translate right 20 percent\n",
        "        offset = np.array([0.0, amount], dtype = np.float32)\n",
        "        size = np.array([IMAGE_SIZE, ceil((1+amount) * IMAGE_SIZE)], dtype = np.int32)\n",
        "        w_start = int(floor((1 - (1+amount)) * IMAGE_SIZE))\n",
        "        w_end = IMAGE_SIZE\n",
        "        h_start = 0\n",
        "        h_end = IMAGE_SIZE\n",
        "    elif index == 2: # Translate top 20 percent\n",
        "        offset = np.array([amount, 0.0], dtype = np.float32)\n",
        "        size = np.array([ceil((1-amount) * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
        "        w_start = 0\n",
        "        w_end = IMAGE_SIZE\n",
        "        h_start = 0\n",
        "        h_end = int(ceil((1-amount) * IMAGE_SIZE)) \n",
        "    else: # Translate bottom 20 percent\n",
        "        offset = np.array([amount, 0.0], dtype = np.float32)\n",
        "        size = np.array([ceil((1+amount) * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
        "        w_start = 0\n",
        "        w_end = IMAGE_SIZE\n",
        "        h_start = int(floor((1 - (1+amount)) * IMAGE_SIZE))\n",
        "        h_end = IMAGE_SIZE \n",
        "        \n",
        "        \n",
        "    offsets = np.zeros((len(X_imgs), 2), dtype = np.float32)\n",
        "    X_translated_arr = []\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        X_translated = np.zeros((len(X_imgs), 32, 32, 3), dtype = np.float32)\n",
        "        X_translated.fill(1.0) # Filling background color\n",
        "        offsets[:, :] = offset \n",
        "        glimpses = tf.image.extract_glimpse(X_imgs, size, offsets)\n",
        "        glimpses = sess.run(glimpses)\n",
        "        X_translated[:, h_start: h_start + size[0], w_start: w_start + size[1], :] = glimpses\n",
        "        X_translated_arr.extend(X_translated)\n",
        "    X_translated_arr = np.array(X_translated_arr, dtype = np.float32)\n",
        "    return X_translated_arr\n",
        "\n",
        "    \n",
        "def translate_images(X_imgs,max_translate):\n",
        "    n_images = len(X_imgs)\n",
        "    X_imgs = np.array(X_imgs,np.float32)\n",
        "    n_translations = 4\n",
        "    x_measures = np.random.randint(0,2*max_translate,1)\n",
        "    x_measures = x_measures - max_translate\n",
        "    x_measures = x_measures + 0.00001\n",
        "    x_measures_abs = np.abs(x_measures)\n",
        "    i = x_measures/x_measures_abs\n",
        "    i = max(i,0)\n",
        "    i = 1-i\n",
        "    X_translated_imgs = get_translate_parameters(X_imgs, i, x_measures/100)\n",
        "    \n",
        "    y_measures = np.random.randint(0,2*max_translate,1)\n",
        "    y_measures = y_measures - max_translate\n",
        "    y_measures = y_measures + 0.00001\n",
        "    y_measures_abs = np.abs(y_measures)\n",
        "    i = y_measures/y_measures_abs\n",
        "    i = 2 + max(i,0)\n",
        "    i = 5-i\n",
        "    Y_translated_imgs = get_translate_parameters(X_translated_imgs, i, y_measures/100)\n",
        "    \n",
        "    return Y_translated_imgs\n",
        "\n",
        "  \n",
        "def flip_images(X_imgs):\n",
        "    X_imgs = np.array(X_imgs,np.float32)\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    X = tf.placeholder(tf.float32, shape=(len(X_imgs), 32, 32, 3))\n",
        "    random_flip = tf.image.random_flip_left_right(X)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      random_flipped_imgs = sess.run(random_flip, feed_dict={X: X_imgs})\n",
        "    \n",
        "    return random_flipped_imgs\n",
        "\n",
        "  \n",
        "def augment(images):\n",
        "  input_x = images\n",
        "  rotated_imgs = rotate_images(input_x, 15)\n",
        "  translated_imgs = np.array(rotated_imgs)\n",
        "  for i in range(len(rotated_imgs)//200):\n",
        "    translated_imgs[200*i:200*i+200] = translate_images(rotated_imgs[200*i:200*i+200],10)\n",
        "  \n",
        "  \n",
        "  flipped_imgs = flip_images(translated_imgs)\n",
        "  \n",
        "  augmented_imgs = flipped_imgs\n",
        "  \n",
        "  return augmented_imgs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dw1LSFvRLO2p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train the network"
      ]
    },
    {
      "metadata": {
        "id": "UKmjI_OsLjXc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import datetime\n",
        "\n",
        "min_cost = 100000.0\n",
        "max_accuracy = 0.0\n",
        "\n",
        "# loop over epochs: \n",
        "epoch = 0 \n",
        "last_best_epoch = -1\n",
        "\n",
        "vary_it = 0\n",
        "\n",
        "while(1):\n",
        "  \n",
        "    \n",
        "    xs_dict['1'] = np.load('xs_arr1.npy')\n",
        "    ys_dict['1'] = np.load('ys_arr1.npy')\n",
        "    xs_dict['2'] = np.load('xs_arr2.npy')\n",
        "    ys_dict['2'] = np.load('ys_arr2.npy')\n",
        "    xs_dict['3'] = np.load('xs_arr3.npy')\n",
        "    ys_dict['3'] = np.load('ys_arr3.npy')\n",
        "    xs_dict['4'] = np.load('xs_arr4.npy')\n",
        "    ys_dict['4'] = np.load('ys_arr4.npy')\n",
        "    xs_dict['5'] = np.load('xs_arr5.npy')\n",
        "    ys_dict['5'] = np.load('ys_arr5.npy')\n",
        "    xs_dict['6'] = np.load('xs_arr6.npy')\n",
        "    ys_dict['6'] = np.load('ys_arr6.npy')\n",
        "    xs_dict['7'] = np.load('xs_arr7.npy')\n",
        "    ys_dict['7'] = np.load('ys_arr7.npy')\n",
        "    xs_dict['8'] = np.load('xs_arr8.npy')\n",
        "    ys_dict['8'] = np.load('ys_arr8.npy')\n",
        "    xs_dict['9'] = np.load('xs_arr9.npy')\n",
        "    ys_dict['9'] = np.load('ys_arr9.npy')\n",
        "    xs_dict['10'] = np.load('xs_arr10.npy')\n",
        "    ys_dict['10'] = np.load('ys_arr10.npy')\n",
        "    xs_dict['11'] = np.load('xs_arr11.npy')\n",
        "    ys_dict['11'] = np.load('ys_arr11.npy')\n",
        "    xs_dict['12'] = np.load('xs_arr12.npy')\n",
        "    ys_dict['12'] = np.load('ys_arr12.npy')\n",
        "    xs_dict['13'] = np.load('xs_arr13.npy')\n",
        "    ys_dict['13'] = np.load('ys_arr13.npy')\n",
        "    xs_dict['14'] = np.load('xs_arr14.npy')\n",
        "    ys_dict['14'] = np.load('ys_arr14.npy')\n",
        "    epoch_arr = np.load('epochs.npy')\n",
        "    cost_arr = np.load('cost.npy')\n",
        "    accuracy_arr = np.load('accuracy.npy')\n",
        "    train_cost_arr = np.load('train_cost.npy')\n",
        "    train_accuracy_arr = np.load('train_accuracy.npy')\n",
        "    \n",
        "    \n",
        "    \n",
        "    nxs_dict['1'] = np.array(xs_dict['1'])\n",
        "    nxs_dict['2'] = np.array(xs_dict['2'])\n",
        "    nxs_dict['3'] = np.array(xs_dict['3'])\n",
        "    nxs_dict['4'] = np.array(xs_dict['4'])\n",
        "    nxs_dict['5'] = np.array(xs_dict['5'])\n",
        "    nxs_dict['6'] = np.array(xs_dict['6'])\n",
        "    nxs_dict['7'] = np.array(xs_dict['7'])\n",
        "    nxs_dict['8'] = np.array(xs_dict['8'])\n",
        "    nxs_dict['9'] = np.array(xs_dict['9'])\n",
        "    nxs_dict['10'] = np.array(xs_dict['10'])\n",
        "    nxs_dict['11'] = np.array(xs_dict['11'])\n",
        "    nxs_dict['12'] = np.array(xs_dict['12'])\n",
        "    nxs_dict['13'] = np.array(xs_dict['13'])\n",
        "    nxs_dict['14'] = np.array(xs_dict['14'])\n",
        "    \n",
        "    nys_dict['1'] = np.array(ys_dict['1'])\n",
        "    nys_dict['2'] = np.array(ys_dict['2'])\n",
        "    nys_dict['3'] = np.array(ys_dict['3'])\n",
        "    nys_dict['4'] = np.array(ys_dict['4'])\n",
        "    nys_dict['5'] = np.array(ys_dict['5'])\n",
        "    nys_dict['6'] = np.array(ys_dict['6'])\n",
        "    nys_dict['7'] = np.array(ys_dict['7'])\n",
        "    nys_dict['8'] = np.array(ys_dict['8'])\n",
        "    nys_dict['9'] = np.array(ys_dict['9'])\n",
        "    nys_dict['10'] = np.array(ys_dict['10'])\n",
        "    nys_dict['11'] = np.array(ys_dict['11'])\n",
        "    nys_dict['12'] = np.array(ys_dict['12'])\n",
        "    nys_dict['13'] = np.array(ys_dict['13'])\n",
        "    nys_dict['14'] = np.array(ys_dict['14'])\n",
        "    \n",
        "    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "  \n",
        "    avg_cost = 0.0 # track cost to monitor performance during training\n",
        "    avg_accuracy_pct = 0.0\n",
        "\n",
        "    # loop over all batches of the epoch:\n",
        "    n_batches = int(train_x.shape[0] / batch_size)\n",
        "\n",
        "    \n",
        "    train_x_og = np.array(train_x)\n",
        "    train_x = augment(train_x)\n",
        "      \n",
        "    for i in range(n_batches):\n",
        "        \n",
        "        nys_dict['1'] = np.zeros(ys_dict['1'].shape[0],dtype=np.float32)\n",
        "        nys_dict['2'] = np.zeros(ys_dict['2'].shape[0],dtype=np.float32)\n",
        "        nys_dict['3'] = np.zeros(ys_dict['3'].shape[0],dtype=np.float32)\n",
        "        nys_dict['4'] = np.zeros(ys_dict['4'].shape[0],dtype=np.float32)\n",
        "        nys_dict['5'] = np.zeros(ys_dict['5'].shape[0],dtype=np.float32)\n",
        "        nys_dict['6'] = np.zeros(ys_dict['6'].shape[0],dtype=np.float32)\n",
        "        nys_dict['7'] = np.zeros(ys_dict['7'].shape[0],dtype=np.float32)\n",
        "        nys_dict['8'] = np.zeros(ys_dict['8'].shape[0],dtype=np.float32)\n",
        "        nys_dict['9'] = np.zeros(ys_dict['9'].shape[0],dtype=np.float32)\n",
        "        nys_dict['10'] = np.zeros(ys_dict['10'].shape[0],dtype=np.float32)\n",
        "        nys_dict['11'] = np.zeros(ys_dict['11'].shape[0],dtype=np.float32)\n",
        "        nys_dict['12'] = np.zeros(ys_dict['12'].shape[0],dtype=np.float32)\n",
        "        nys_dict['13'] = np.zeros(ys_dict['13'].shape[0],dtype=np.float32)\n",
        "        nys_dict['14'] = np.zeros(ys_dict['14'].shape[0],dtype=np.float32)\n",
        "        \n",
        "        for ij in range(1,15):\n",
        "          m_dict[str(ij)] = np.zeros(xs_dict[str(ij)].shape[0] - 1, dtype=np.float32)\n",
        "          c_dict[str(ij)] = np.zeros(xs_dict[str(ij)].shape[0] - 1, dtype=np.float32)\n",
        "\n",
        "          for i in range(xs_dict[str(ij)].shape[0] - 1):\n",
        "              m_dict[str(ij)][i] = (ys_dict[str(ij)][i + 1] - ys_dict[str(ij)][i]) / (xs_dict[str(ij)][i + 1] - xs_dict[str(ij)][i])\n",
        "              c_dict[str(ij)][i] = ys_dict[str(ij)][i] - m_dict[str(ij)][i] * xs_dict[str(ij)][i]\n",
        "\n",
        "        \n",
        "        \n",
        "        batch_x, batch_y = next_batch(batch_size,train_x,train_y)\n",
        "        _, batch_cost, batch_acc, wd, bd = session.run([optimizer, cost, accuracy_pct,weight_dict,bias_dict], \n",
        "                                               feed_dict={x: batch_x, y: batch_y,do_dropout: np.array([1.0],dtype=np.float32)})\n",
        "        avg_cost += batch_cost / n_batches\n",
        "        avg_accuracy_pct += batch_acc / n_batches\n",
        "        ys_dict['1'] = nys_dict['1']\n",
        "        ys_dict['2'] = nys_dict['2']\n",
        "        ys_dict['3'] = nys_dict['3']\n",
        "        ys_dict['4'] = nys_dict['4']\n",
        "        ys_dict['5'] = nys_dict['5']\n",
        "        ys_dict['6'] = nys_dict['6']\n",
        "        ys_dict['7'] = nys_dict['7']\n",
        "        ys_dict['8'] = nys_dict['8']\n",
        "        ys_dict['9'] = nys_dict['9']\n",
        "        ys_dict['10'] = nys_dict['10']\n",
        "        ys_dict['11'] = nys_dict['11']\n",
        "        ys_dict['12'] = nys_dict['12']\n",
        "        ys_dict['13'] = nys_dict['13']\n",
        "        ys_dict['14'] = nys_dict['14']\n",
        "     \n",
        "    train_x = np.array(train_x_og)\n",
        "    \n",
        "    # output logs at end of each epoch of training:\n",
        "    print(\"Epoch \", '%03d' % (epoch+1), \n",
        "          \": cost = \", '{:.3f}'.format(avg_cost), \n",
        "          \", accuracy = \", '{:.2f}'.format(avg_accuracy_pct), \"%\", \n",
        "          sep='')\n",
        "    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    \n",
        "    for ij in range(1,15):\n",
        "          m_dict[str(ij)] = np.zeros(xs_dict[str(ij)].shape[0] - 1, dtype=np.float32)\n",
        "          c_dict[str(ij)] = np.zeros(xs_dict[str(ij)].shape[0] - 1, dtype=np.float32)\n",
        "\n",
        "          for i in range(xs_dict[str(ij)].shape[0] - 1):\n",
        "              m_dict[str(ij)][i] = (ys_dict[str(ij)][i + 1] - ys_dict[str(ij)][i]) / (xs_dict[str(ij)][i + 1] - xs_dict[str(ij)][i])\n",
        "              c_dict[str(ij)][i] = ys_dict[str(ij)][i] - m_dict[str(ij)][i] * xs_dict[str(ij)][i]\n",
        "\n",
        "    \n",
        "    t_batches = int(val_x.shape[0] / 100)\n",
        "\n",
        "    test_cost = 0\n",
        "    test_accuracy_pct = 0\n",
        "\n",
        "    for i in range(t_batches):\n",
        "\n",
        "      img,lbl = next_batch(100,val_x,val_y)\n",
        "      interim_test_cost = session.run(cost,feed_dict={x: img, y: lbl,do_dropout: np.array([0.0],dtype=np.float32)})\n",
        "      interim_test_accuracy_pct = session.run(accuracy_pct,feed_dict={x: img, y: lbl,do_dropout: np.array([0.0],dtype=np.float32)})\n",
        "\n",
        "      test_cost+=(interim_test_cost/(t_batches))\n",
        "      test_accuracy_pct+=(interim_test_accuracy_pct/(t_batches))\n",
        "\n",
        "\n",
        "    epoch_arr = np.append(epoch_arr,epoch)\n",
        "    cost_arr = np.append(cost_arr,test_cost)    \n",
        "    accuracy_arr = np.append(accuracy_arr,test_accuracy_pct)\n",
        "    \n",
        "    train_cost_arr = np.append(train_cost_arr,avg_cost)\n",
        "    train_accuracy_arr = np.append(train_accuracy_arr,avg_accuracy_pct)\n",
        "    \n",
        "\n",
        "    print(\"Validation Cost:\", '{:.3f}'.format(test_cost),\"  Validation Accuracy: \", '{:.2f}'.format(test_accuracy_pct), \"%\", sep='')\n",
        "    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_c1.npy\",wd['W_c1'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_c2.npy\",wd['W_c2'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_c3.npy\",wd['W_c3'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_c4.npy\",wd['W_c4'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_c5.npy\",wd['W_c5'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_c6.npy\",wd['W_c6'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_c7.npy\",wd['W_c7'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_c8.npy\",wd['W_c8'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_c9.npy\",wd['W_c9'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_c10.npy\",wd['W_c10'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_c11.npy\",wd['W_c11'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_c12.npy\",wd['W_c12'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_c13.npy\",wd['W_c13'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_d1.npy\",wd['W_d1'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"W_out.npy\",wd['W_out'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_c1.npy\",bd['b_c1'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_c2.npy\",bd['b_c2'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_c3.npy\",bd['b_c3'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_c4.npy\",bd['b_c4'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_c5.npy\",bd['b_c5'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_c6.npy\",bd['b_c6'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_c7.npy\",bd['b_c7'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_c8.npy\",bd['b_c8'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_c9.npy\",bd['b_c9'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_c10.npy\",bd['b_c10'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_c11.npy\",bd['b_c11'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_c12.npy\",bd['b_c12'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_c13.npy\",bd['b_c13'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_d1.npy\",bd['b_d1'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"b_out.npy\",bd['b_out'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"xs_arr1.npy\",xs_dict['1'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"ys_arr1.npy\",ys_dict['1'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"xs_arr2.npy\",xs_dict['2'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"ys_arr2.npy\",ys_dict['2'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"xs_arr3.npy\",xs_dict['3'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"ys_arr3.npy\",ys_dict['3'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"xs_arr4.npy\",xs_dict['4'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"ys_arr4.npy\",ys_dict['4'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"xs_arr5.npy\",xs_dict['5'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"ys_arr5.npy\",ys_dict['5'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"xs_arr6.npy\",xs_dict['6'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"ys_arr6.npy\",ys_dict['6'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"xs_arr7.npy\",xs_dict['7'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"ys_arr7.npy\",ys_dict['7'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"xs_arr8.npy\",xs_dict['8'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"ys_arr8.npy\",ys_dict['8'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"xs_arr9.npy\",xs_dict['9'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"ys_arr9.npy\",ys_dict['9'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"xs_arr10.npy\",xs_dict['10'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"ys_arr10.npy\",ys_dict['10'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"xs_arr11.npy\",xs_dict['11'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"ys_arr11.npy\",ys_dict['11'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"xs_arr12.npy\",xs_dict['12'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"ys_arr12.npy\",ys_dict['12'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"xs_arr13.npy\",xs_dict['13'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"ys_arr13.npy\",ys_dict['13'])\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"xs_arr14.npy\",xs_dict['14'])\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"ys_arr14.npy\",ys_dict['14'])\n",
        "\n",
        "\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"epochs.npy\",epoch_arr)\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"cost.npy\",cost_arr)\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"accuracy.npy\",accuracy_arr)\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"traincost.npy\",train_cost_arr)\n",
        "    np.save(\"VAL\"+str(test_accuracy_pct*100)[0:4]+\"trainaccuracy.npy\",train_accuracy_arr)\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "    min_cost = min(min_cost,avg_cost)\n",
        "    max_accuracy = max(max_accuracy,test_accuracy_pct)\n",
        "    print(\"Min Train Cost:\", '{:.3f}'.format(min_cost),\"  Max Validation Accuracy: \", '{:.2f}'.format(max_accuracy), \"%\", sep='')\n",
        "\n",
        "    if(epoch%10==0):\n",
        "        plt.subplot(2,2,1)\n",
        "        plt.title(\"Activation 1\")\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.plot(xs_dict['1'],ys_dict['1'],'b')\n",
        "\n",
        "        plt.subplot(2,2,2)\n",
        "        plt.title(\"Activation 2\")\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.plot(xs_dict['2'],ys_dict['2'],'b')\n",
        "\n",
        "        plt.subplot(2,2,3)\n",
        "        plt.title(\"Activation 3\")\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.plot(xs_dict['3'],ys_dict['3'],'b')\n",
        "\n",
        "        plt.subplot(2,2,4)\n",
        "        plt.title(\"Activation 4\")\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.plot(xs_dict['4'],ys_dict['4'],'b')\n",
        "        plt.show()\n",
        "\n",
        "        plt.subplot(2,2,1)\n",
        "        plt.title(\"Activation 5\")\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.plot(xs_dict['5'],ys_dict['5'],'b')\n",
        "\n",
        "        plt.subplot(2,2,2)\n",
        "        plt.title(\"Activation 6\")\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.plot(xs_dict['6'],ys_dict['6'],'b')\n",
        "\n",
        "        plt.subplot(2,2,3)\n",
        "        plt.title(\"Activation 7\")\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.plot(xs_dict['7'],ys_dict['7'],'b')\n",
        "\n",
        "        plt.subplot(2,2,4)\n",
        "        plt.title(\"Activation 8\")\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.plot(xs_dict['8'],ys_dict['8'],'b')\n",
        "        plt.show()\n",
        "\n",
        "        plt.subplot(2,2,1)\n",
        "        plt.title(\"Activation 9\")\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.plot(xs_dict['9'],ys_dict['9'],'b')\n",
        "\n",
        "        plt.subplot(2,2,2)\n",
        "        plt.title(\"Activation 10\")\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.plot(xs_dict['10'],ys_dict['10'],'b')\n",
        "\n",
        "        plt.subplot(2,2,3)\n",
        "        plt.title(\"Activation 11\")\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.plot(xs_dict['11'],ys_dict['11'],'b')\n",
        "\n",
        "        plt.subplot(2,2,4)\n",
        "        plt.title(\"Activation 12\")\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.plot(xs_dict['12'],ys_dict['12'],'b')\n",
        "        plt.show()\n",
        "\n",
        "        plt.subplot(2,2,1)\n",
        "        plt.title(\"Activation 13\")\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.plot(xs_dict['13'],ys_dict['13'],'b')\n",
        "\n",
        "        plt.subplot(2,2,2)\n",
        "        plt.title(\"Activation 14\")\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.plot(xs_dict['14'],ys_dict['14'],'b')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    \n",
        "    if (epoch-last_best_epoch) > epoch_no_change:\n",
        "      break\n",
        "    \n",
        "    np.save(\"W_c1.npy\",wd['W_c1'])\n",
        "    np.save(\"W_c2.npy\",wd['W_c2'])\n",
        "    np.save(\"W_c3.npy\",wd['W_c3'])\n",
        "    np.save(\"W_c4.npy\",wd['W_c4'])\n",
        "    np.save(\"W_c5.npy\",wd['W_c5'])\n",
        "    np.save(\"W_c6.npy\",wd['W_c6'])\n",
        "    np.save(\"W_c7.npy\",wd['W_c7'])\n",
        "    np.save(\"W_c8.npy\",wd['W_c8'])\n",
        "    np.save(\"W_c9.npy\",wd['W_c9'])\n",
        "    np.save(\"W_c10.npy\",wd['W_c10'])\n",
        "    np.save(\"W_c11.npy\",wd['W_c11'])\n",
        "    np.save(\"W_c12.npy\",wd['W_c12'])\n",
        "    np.save(\"W_c13.npy\",wd['W_c13'])\n",
        "    np.save(\"W_d1.npy\",wd['W_d1'])\n",
        "    np.save(\"W_out.npy\",wd['W_out'])\n",
        "    np.save(\"b_c1.npy\",bd['b_c1'])\n",
        "    np.save(\"b_c2.npy\",bd['b_c2'])\n",
        "    np.save(\"b_c3.npy\",bd['b_c3'])\n",
        "    np.save(\"b_c4.npy\",bd['b_c4'])\n",
        "    np.save(\"b_c5.npy\",bd['b_c5'])\n",
        "    np.save(\"b_c6.npy\",bd['b_c6'])\n",
        "    np.save(\"b_c7.npy\",bd['b_c7'])\n",
        "    np.save(\"b_c8.npy\",bd['b_c8'])\n",
        "    np.save(\"b_c9.npy\",bd['b_c9'])\n",
        "    np.save(\"b_c10.npy\",bd['b_c10'])\n",
        "    np.save(\"b_c11.npy\",bd['b_c11'])\n",
        "    np.save(\"b_c12.npy\",bd['b_c12'])\n",
        "    np.save(\"b_c13.npy\",bd['b_c13'])\n",
        "    np.save(\"b_d1.npy\",bd['b_d1'])\n",
        "    np.save(\"b_out.npy\",bd['b_out'])\n",
        "    np.save(\"xs_arr1.npy\",xs_dict['1'])\n",
        "    np.save(\"ys_arr1.npy\",ys_dict['1'])\n",
        "    np.save(\"xs_arr2.npy\",xs_dict['2'])\n",
        "    np.save(\"ys_arr2.npy\",ys_dict['2'])\n",
        "    np.save(\"xs_arr3.npy\",xs_dict['3'])\n",
        "    np.save(\"ys_arr3.npy\",ys_dict['3'])\n",
        "    np.save(\"xs_arr4.npy\",xs_dict['4'])\n",
        "    np.save(\"ys_arr4.npy\",ys_dict['4'])\n",
        "    np.save(\"xs_arr5.npy\",xs_dict['5'])\n",
        "    np.save(\"ys_arr5.npy\",ys_dict['5'])\n",
        "    np.save(\"xs_arr6.npy\",xs_dict['6'])\n",
        "    np.save(\"ys_arr6.npy\",ys_dict['6'])\n",
        "    np.save(\"xs_arr7.npy\",xs_dict['7'])\n",
        "    np.save(\"ys_arr7.npy\",ys_dict['7'])\n",
        "    np.save(\"xs_arr8.npy\",xs_dict['8'])\n",
        "    np.save(\"ys_arr8.npy\",ys_dict['8'])\n",
        "    np.save(\"xs_arr9.npy\",xs_dict['9'])\n",
        "    np.save(\"ys_arr9.npy\",ys_dict['9'])\n",
        "    np.save(\"xs_arr10.npy\",xs_dict['10'])\n",
        "    np.save(\"ys_arr10.npy\",ys_dict['10'])\n",
        "    np.save(\"xs_arr11.npy\",xs_dict['11'])\n",
        "    np.save(\"ys_arr11.npy\",ys_dict['11'])\n",
        "    np.save(\"xs_arr12.npy\",xs_dict['12'])\n",
        "    np.save(\"ys_arr12.npy\",ys_dict['12'])\n",
        "    np.save(\"xs_arr13.npy\",xs_dict['13'])\n",
        "    np.save(\"ys_arr13.npy\",ys_dict['13'])\n",
        "    np.save(\"xs_arr14.npy\",xs_dict['14'])\n",
        "    np.save(\"ys_arr14.npy\",ys_dict['14'])\n",
        "    np.save(\"epochs.npy\",epoch_arr)\n",
        "    np.save(\"cost.npy\",cost_arr)\n",
        "    np.save(\"accuracy.npy\",accuracy_arr)\n",
        "    np.save(\"train_cost.npy\",train_cost_arr)\n",
        "    np.save(\"train_accuracy.npy\",train_accuracy_arr)\n",
        "    \n",
        "      \n",
        "    epoch = epoch+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "15tPgRb8LjXb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Visualize Loss and Accuracy"
      ]
    },
    {
      "metadata": {
        "id": "ILP-_4gE2pzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  plt.subplot(1,2,1)\n",
        "  plt.title(\"Cost\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Cost\")\n",
        "  plt.plot(epoch_arr[1:],train_cost_arr[1:],'b')\n",
        "  plt.plot(epoch_arr[1:],cost_arr[1:],'g')\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.title(\"Accuracy\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.plot(epoch_arr[1:],train_accuracy_arr[1:],'b')\n",
        "  plt.plot(epoch_arr[1:],accuracy_arr[1:],'g')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pFb-vtJ3PLH2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Test Model"
      ]
    },
    {
      "metadata": {
        "id": "dq3x0Z2VNpAR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Testing already done in every epoch.\n",
        "print(\"Training Complete. Testing Model.\\n\")\n",
        "\n",
        "test_x = np.array(test_x)\n",
        "t_batches = int(test_x.shape[0] / 100)\n",
        "\n",
        "test_cost = 0\n",
        "test_accuracy_pct = 0\n",
        "\n",
        "for i in range(t_batches):\n",
        "\n",
        "  img,lbl = next_batch(100,test_x,test_y)\n",
        "  interim_test_cost, interim_test_accuracy_pct = session.run([cost, accuracy_pct], feed_dict={x: img, y: lbl, do_dropout: np.array([0.0],dtype=np.float32)})\n",
        "  test_cost+=(interim_test_cost/t_batches)\n",
        "  test_accuracy_pct+=(interim_test_accuracy_pct/t_batches)\n",
        "\n",
        "print(\"Test Cost:\", '{:.3f}'.format(test_cost))\n",
        "print(\"Test Accuracy: \", '{:.2f}'.format(test_accuracy_pct), \"%\", sep='')\n",
        "\n",
        "session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}